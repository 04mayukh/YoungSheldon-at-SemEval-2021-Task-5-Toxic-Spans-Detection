{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "toxic spans github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Sn4TLvtLnq"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2X6WFl6tMgo"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKKtlwCNcYTs"
      },
      "source": [
        "# 4.2.1\r\n",
        "!pip install transformers==4.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItanH204cbiD"
      },
      "source": [
        "pip install tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc-9B1RYcPPD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "from ast import literal_eval"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBrVq4sqcPPJ"
      },
      "source": [
        "# Train Data\n",
        "df_train = pd.read_csv('tsd_train.csv',dtype=str, encoding='utf-8', index_col=False)\n",
        "df_train[\"spans\"] = df_train.spans.apply(literal_eval)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuJ2FsBTcPPL"
      },
      "source": [
        "# Trial Data *Replace the file by test file for prediction*\n",
        "df_trial = pd.read_csv('tsd_trial.csv',dtype=str, encoding='utf-8', index_col=False)\n",
        "df_trial[\"spans\"] = df_trial.spans.apply(literal_eval)\n",
        "df_trial.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaaTuxm_cPPL"
      },
      "source": [
        "# Train data\n",
        "lines = df_train[\"text\"]\n",
        "lines_original = df_train[\"text\"]\n",
        "tags = df_train[\"spans\"]\n",
        "\n",
        "#Trial Data\n",
        "lines_trial = df_trial[\"text\"]\n",
        "lines_original_trial = df_trial[\"text\"]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62-gF50ncPPL"
      },
      "source": [
        "# Returns position of each tag indices seperated from the single tag indices array\n",
        "def tag_index(indices):\n",
        "    indices_sep = []\n",
        "    temp = []\n",
        "    for i in range(0,len(indices)-1):\n",
        "        temp.append(int(indices[i]))\n",
        "        \n",
        "        if(i == len(indices)-2):\n",
        "            temp.append(int(indices[i+1]))\n",
        "            indices_sep.append(temp)\n",
        "        \n",
        "        if(int(indices[i]) + 1 != int(indices[i+1])):\n",
        "            if(i == len(indices)-1):\n",
        "                print(\"Error\")\n",
        "                \n",
        "            indices_sep.append(temp)\n",
        "            temp = []\n",
        "    return indices_sep"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5x-SblmcPPL"
      },
      "source": [
        "# each row contains array of character position of each tag\n",
        "tag_indices = tags.apply(lambda tag: tag_index(tag))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PowpwhRGcPPM",
        "outputId": "caf38c7a-16a2-4609-eca2-d397074340ca"
      },
      "source": [
        "# Viewing toxic spans after seperation\n",
        "index = 6\n",
        "u_temp =tag_indices[index]\n",
        "print(lines[index])\n",
        "print(\"No. of toxic words: \",len(u_temp))\n",
        "t_temp = (lines[index])\n",
        "for y_temp in u_temp:\n",
        "  print(t_temp[y_temp[0]:y_temp[-1]+1])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please people, stop using these silly, stupid emoticons.\n",
            "No. of toxic words:  2\n",
            "stupid\n",
            "emoticons\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Mlka4fcPPM"
      },
      "source": [
        "# Removes punctuation. Used later\n",
        "def remove_punctuation(text):\n",
        "    return re.sub('^[\"(.,*\\\\!)\\[\\]*?\"\"]{1,2}|[\"(.,*\\\\!)\\[\\]*?\"\"]{1,3}$', '', text) \n",
        "\n",
        "# Finds the toxic word from its array of indices\n",
        "def toxic_word(line, tag_indice):\n",
        "    toxic_words = []\n",
        "    for i in range(0,len(tag_indice)):\n",
        "        index_range = tag_indice[i]\n",
        "        start = index_range[0]\n",
        "        end = index_range[-1] + 1 \n",
        "        words = line[start:end]\n",
        "        for word in words.split():\n",
        "            word = word.lower()\n",
        "            word = remove_punctuation(word)\n",
        "            toxic_words.append(word)\n",
        "    return toxic_words"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRb4TVS9cPPN"
      },
      "source": [
        "# Testing toxic word seperation\n",
        "index = 0\n",
        "toxic_word(lines[index],tag_indices[index]) # lines contains the string. tag_indices contains independent toxic word positions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTveFbJYcPPN"
      },
      "source": [
        "toxic_words = []\n",
        "for i in range(0,len(lines)):\n",
        "    line = lines[i]\n",
        "    tag_indice = tag_indices[i]\n",
        "    toxic_words.append(toxic_word(line, tag_indice))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySE41RugcPPN"
      },
      "source": [
        "toxic_words[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMsmpgmQcPPN"
      },
      "source": [
        "<h5> Finding toxic words is done. Now we need to locate them in lines to convert it into a token classification task.</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmZ0WJsIcPPO"
      },
      "source": [
        "def remove_whitespace(text):\n",
        "    whitespace = re.compile(r\"\\s+\")\n",
        "    return whitespace.sub(\" \", text).strip()\n",
        "\n",
        "def remove_ascii(text):\n",
        "    return (text.encode('ascii', 'ignore')).decode(\"utf-8\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79QEVkHTcPPO"
      },
      "source": [
        "# lowercasing, removing white spaces, removing ascii characters to input into our model\n",
        "# Train data\n",
        "lines = lines.apply(lambda text: text.lower())\n",
        "lines = lines.apply(lambda text: remove_whitespace(text))\n",
        "lines = lines.apply(lambda text: remove_ascii(text))\n",
        "print(lines[2602])\n",
        "print(\"**********************************************\")\n",
        "lines_original = lines_original.apply(lambda text: text.lower())\n",
        "print(lines_original[2602])\n",
        "\n",
        "print(\"\\n#################################################################################################################\\n\")\n",
        "\n",
        "#trial data\n",
        "lines_trial = lines_trial.apply(lambda text: text.lower())\n",
        "lines_trial = lines_trial.apply(lambda text: remove_whitespace(text))\n",
        "lines_trial = lines_trial.apply(lambda text: remove_ascii(text))\n",
        "print(lines_trial[260])\n",
        "print(\"**********************************************\")\n",
        "lines_original_trial = lines_original_trial.apply(lambda text: text.lower())\n",
        "print(lines_original_trial[260])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRFrcKbLcPPO"
      },
      "source": [
        "#Splitting lines into words\n",
        "#Train Data\n",
        "lines_split = lines.apply(lambda text: text.split())\n",
        "print(len(lines_split[0]))\n",
        "print(lines_split[0])\n",
        "\n",
        "print(\"\\n#################################################################################################################\\n\")\n",
        "\n",
        "#Trial Data\n",
        "lines_split_trial = lines_trial.apply(lambda text: text.split())\n",
        "print(len(lines_split_trial[260]))\n",
        "print(lines_split_trial[260])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9vR3qgrcPPP"
      },
      "source": [
        "# For empty strings that get stored in some places after remove punctuation step carried next.\n",
        "def data_leak(word):\n",
        "    if word == '':\n",
        "        word = \"p\"\n",
        "    return word"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-V8ezqcPPP"
      },
      "source": [
        "# Cleaning seperated words\n",
        "def split_filter(text):\n",
        "    text_array = pd.Series(text)\n",
        "    text_array = text_array.apply(lambda word: remove_punctuation(word))\n",
        "    \n",
        "    # data problem solution casued by removing punctuation as some strings were purely punctuation.\n",
        "    text_array = text_array.apply(lambda word: data_leak(word))\n",
        "    \n",
        "    if(len(text) != len(text_array)):\n",
        "        print(\"Length mismatch\")\n",
        "    return np.asarray(text_array)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFzqj_pUcPPP"
      },
      "source": [
        "# Train data\n",
        "lines_split_no_punct = lines_split.apply(lambda l: split_filter(l))\n",
        "print(lines_split[0])\n",
        "print(lines_split_no_punct[0])\n",
        "\n",
        "print(\"\\n#################################################################################################################\\n\")\n",
        "\n",
        "#Trial data\n",
        "lines_split_no_punct_trial = lines_split_trial.apply(lambda l: split_filter(l))\n",
        "print(lines_split_trial[260])\n",
        "print(lines_split_no_punct_trial[260])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yL_karxcPPP"
      },
      "source": [
        "<h5>Text pre-processing is done. We now create labels using the toxic words found on cleaned text</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4XPKTQxcPPP"
      },
      "source": [
        "def create_label(toxic_word, word_array):\n",
        "    words = list(set(toxic_word))\n",
        "    label = np.zeros((len(word_array)))\n",
        "    for word in words:\n",
        "        positions = (np.where(word_array == word))\n",
        "        for position in positions:\n",
        "            label[position] = 1\n",
        "    return label"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbkFRRYRcPPQ"
      },
      "source": [
        "train_tags = []\n",
        "for i in range(0,len(lines_split_no_punct)):\n",
        "    tag = create_label(toxic_words[i], lines_split_no_punct[i]) \n",
        "    train_tags.append(tag)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVv-Hb2JcPPQ"
      },
      "source": [
        "index = 2\n",
        "print(lines_split_no_punct[index])\n",
        "print(train_tags[index])\n",
        "print(toxic_words[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtNIoJsmcPPR"
      },
      "source": [
        "# Converting from np array to lists for tokenizer\n",
        "# Train Data\n",
        "train_texts = list(lines_split_no_punct)\n",
        "for i in range(0,len(train_texts)):\n",
        "    train_texts[i] = list(train_texts[i])\n",
        "print(\"Length of Train data: \",np.shape(train_texts))\n",
        "\n",
        "# Trial Data\n",
        "trial_texts = list(lines_split_no_punct_trial)\n",
        "for i in range(0,len(trial_texts)):\n",
        "    trial_texts[i] = list(trial_texts[i])\n",
        "print(\"Length of Trial data: \",np.shape(trial_texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWD5BI2cPPR"
      },
      "source": [
        "# Finding length of sequences (hyper parameter for neural network.)\n",
        "u = lambda text: len(text.split(\" \"))\n",
        "sentence_lengths = []\n",
        "for x in train_texts:\n",
        "    sentence_lengths.append(len(x))\n",
        "print(sorted(sentence_lengths)[-50:])\n",
        "print(len(sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iPcPY4FcPPR"
      },
      "source": [
        "<h4>Token classification task</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kra0Y7nTcPPR"
      },
      "source": [
        "from transformers import TFMPNetModel, MPNetTokenizerFast, XLNetTokenizerFast, TFXLNetModel, AlbertTokenizerFast, TFMT5EncoderModel, TFAlbertModel, TFT5EncoderModel, T5TokenizerFast, TFT5Model, RobertaTokenizerFast, TFRobertaModel, AutoTokenizer, TFXLMRobertaModel, TFBertModel, BertTokenizerFast, TFElectraModel, ElectraTokenizerFast\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcL5tlqPcPPR"
      },
      "source": [
        "# Use tokenizer as required. Remove add_prefic_space for other tokenizers apart from roberta\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "# Train set\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Trial set (max length is set for different tokenizers some returned less than 250)\n",
        "trial_encodings = tokenizer(trial_texts, max_length=250, is_split_into_words=True, padding=\"max_length\", truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGVC_pzqcPPS"
      },
      "source": [
        "print(np.shape(train_encodings.input_ids))\n",
        "print(np.shape(trial_encodings.input_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dcDt-yhcPPS"
      },
      "source": [
        "# Make labels compatible as per tokeniser split and returns training masks for prediction.\n",
        "def encode_tags(tags, encodings):\n",
        "\n",
        "    label_all_tokens = False\n",
        "    encoded_labels = []\n",
        "    masks = []\n",
        "    \n",
        "    for i in range(0, len(tags)):\n",
        "        if( i%1000 == 0):\n",
        "          print(str(i) + \"...\")\n",
        "          \n",
        "        label = tags[i]\n",
        "#         print(label)\n",
        "        word_ids = encodings[i].word_ids\n",
        "#         print(word_ids)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        mask_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "                mask_ids.append(0)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "                mask_ids.append(1)\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "                mask_ids.append(label[word_idx] if label_all_tokens else 0)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "            \n",
        "#         print(label_ids)\n",
        "#         print(mask_ids)\n",
        "#         print()\n",
        "\n",
        "        encoded_labels.append(label_ids)\n",
        "        masks.append(mask_ids)\n",
        "\n",
        "\n",
        "    return (encoded_labels, masks)\n",
        "\n",
        "train_labels, train_masks = encode_tags(train_tags, train_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEhe7znDcPPS"
      },
      "source": [
        "index = 3\n",
        "print(len(train_tags[index]))\n",
        "print(len(train_texts[index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPwleKMycPPT"
      },
      "source": [
        "# Returns masks for trial/test data as per tokenizer\n",
        "def get_masks(texts, encodings):\n",
        "\n",
        "    label_all_tokens = False\n",
        "    masks = []\n",
        "    \n",
        "    for i in range(0, len(texts)):\n",
        "        if(i%100 == 0):\n",
        "          print(i,\"...\")\n",
        "\n",
        "        word_ids = encodings[i].word_ids\n",
        "#         print(word_ids)\n",
        "        previous_word_idx = None\n",
        "        mask_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                mask_ids.append(0)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                mask_ids.append(1)\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                mask_ids.append(label[word_idx] if label_all_tokens else 0)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "            \n",
        "#         print(mask_ids)\n",
        "#         print()\n",
        "\n",
        "        masks.append(mask_ids)\n",
        "\n",
        "\n",
        "    return (masks)\n",
        "trial_masks = get_masks(trial_texts, trial_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxPW7pDucPPT"
      },
      "source": [
        "# Test function for lengths\n",
        "for i in range(0,len(train_encodings.input_ids)):\n",
        "    if(len(train_encodings.input_ids[i]) != len(train_labels[i])):\n",
        "        print(i)\n",
        "        \n",
        "for i in range(0,len(trial_encodings.input_ids)):\n",
        "    if(len(trial_encodings.input_ids[i]) != len(trial_masks[i])):\n",
        "        print(i)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW6OdFIpcPPT"
      },
      "source": [
        "# Train Data\n",
        "truncated_train = np.asarray(train_encodings.input_ids)[:,:250]\n",
        "truncated_train_labels = np.asarray(train_labels)[:,:250]\n",
        "truncated_train_masks = np.asarray(train_masks)[:,:250]\n",
        "\n",
        "# Trial Data\n",
        "truncated_trial = np.asarray(trial_encodings.input_ids)[:,:250]\n",
        "truncated_trial_masks = np.asarray(trial_masks)[:,:250]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uoAwYLPlj7d"
      },
      "source": [
        "attention_masks_train = np.asarray(train_encodings.attention_mask)[:,:250]\r\n",
        "attention_masks_trial = np.asarray(trial_encodings.attention_mask)[:,:250]\r\n",
        "print(np.shape(attention_masks_train))\r\n",
        "print(np.shape(attention_masks_trial))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyi2QrwYcPPT"
      },
      "source": [
        "# Train Data\n",
        "index = 0\n",
        "print(train_texts[index])\n",
        "print(toxic_words[index])\n",
        "print(truncated_train_labels[index,:25])\n",
        "print(truncated_train_masks[index,:25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNrh9Qa3pQJz"
      },
      "source": [
        "# Trial data\r\n",
        "index = 0\r\n",
        "print(trial_texts[index])\r\n",
        "# print(toxic_words[index])\r\n",
        "# print(truncated_train_labels[index,:40])\r\n",
        "print(truncated_trial_masks[index,:60])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtmKIJi7cPPU"
      },
      "source": [
        "# Train Data\n",
        "print(np.shape(truncated_train))\n",
        "print(np.shape(truncated_train_labels))\n",
        "print(np.shape(truncated_train_masks))\n",
        "\n",
        "# Trial Data\n",
        "print(np.shape(truncated_trial))\n",
        "# print(np.shape(truncated_train_labels))\n",
        "print(np.shape(truncated_trial_masks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1hzdnRcPPU"
      },
      "source": [
        "<h5> Model </h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZsfsGdpty8e"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmOkc6E38thj"
      },
      "source": [
        "# Bert, electra, roberta, XLM-Roberta Model, XLnet\r\n",
        "def toxic_span(input_shape):\r\n",
        "    #Model\r\n",
        "    inputs = keras.Input(shape=input_shape, dtype='int32')\r\n",
        "\r\n",
        "    # Import model as required \r\n",
        "    model = TFRobertaModel.from_pretrained('roberta-base')\r\n",
        "    layer = model.layers[0]\r\n",
        "    output = layer(inputs)[0]\r\n",
        "    output = keras.layers.BatchNormalization()(output)\r\n",
        "    output = keras.layers.Dropout(0.1)(output)\r\n",
        "\r\n",
        "    dense = keras.layers.Dense(1, activation=\"sigmoid\")\r\n",
        "    answer = keras.layers.TimeDistributed(dense)(output)\r\n",
        "\r\n",
        "    model = keras.Model(inputs=inputs, outputs=answer, name='toxic_span')\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R1NgTKOxSlF"
      },
      "source": [
        "from tensorflow.keras import backend as K\r\n",
        "def custom_loss(y_true, y_pred):\r\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\r\n",
        "    isMask = tf.math.not_equal(y_true, -100)\r\n",
        "    mask = tf.cast(isMask, dtype=tf.float32)\r\n",
        "    y_true_mask = tf.math.multiply(mask,tf.cast(y_true, dtype=tf.float32))\r\n",
        "    y_pred_mask = tf.math.multiply(mask,y_pred)\r\n",
        "    loss = bce(y_true, y_pred)\r\n",
        "    loss_masked = bce(y_true_mask, y_pred_mask) * 10\r\n",
        "    return loss_masked"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbxdhwCSxV2w"
      },
      "source": [
        "# Set up epochs and steps\r\n",
        "epochs = 4\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "train_data_size = len(truncated_train)\r\n",
        "steps_per_epoch = int(train_data_size / batch_size)\r\n",
        "num_train_steps = steps_per_epoch * epochs\r\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\r\n",
        "\r\n",
        "# creates an optimizer with learning rate schedule\r\n",
        "optimizer = nlp.optimization.create_optimizer(\r\n",
        "    5e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgjiJ-QWt-pb"
      },
      "source": [
        "with strategy.scope():\r\n",
        "    model = toxic_span((250,))\r\n",
        "    optimizer = optimizer\r\n",
        "    loss_fun = custom_loss\r\n",
        "    model.compile(optimizer=optimizer, loss=loss_fun)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AcSLm1hcPPV"
      },
      "source": [
        "# model_ = toxic_span((250,), bert_layer)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-W6Ah9ecPPV",
        "outputId": "adb707cf-ff54-4957-bd71-40bbaef55f07"
      },
      "source": [
        "len(truncated_train)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7939"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQLddjYJcPPW"
      },
      "source": [
        "<h5> Custom evaluation metric </h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMIlvDJUcPPW"
      },
      "source": [
        "def get_predicted_words(train_prediction, train_texts, truncated_train_masks):\n",
        "    predicted_labels = []\n",
        "    predicted_toxic_words = []\n",
        "    round_pred = np.round(train_prediction)\n",
        "    train_texts = np.asarray(train_texts)\n",
        "    for i in range(0,len(truncated_train_masks)):\n",
        "    #     print(i)\n",
        "        pred_label = np.zeros(len(train_texts[i]))\n",
        "        pred_label = round_pred[i][(truncated_train_masks[i,:] == 1)]\n",
        "        pred_label = np.squeeze(pred_label, axis=-1)\n",
        "        predicted_labels.append(pred_label)\n",
        "\n",
        "        pred_toxic_words = []\n",
        "        for j in range(0,len(pred_label)):\n",
        "            if (pred_label[j] == 1):\n",
        "                pred_toxic_words.append(train_texts[i][j])\n",
        "\n",
        "        predicted_toxic_words.append(pred_toxic_words)\n",
        "        \n",
        "    return (predicted_labels, predicted_toxic_words)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB35peqjcPPW"
      },
      "source": [
        "def get_char_positions(lines_original, predicted_toxic_words):\n",
        "    \n",
        "    char_positions = []\n",
        "    for i in range(0,len(lines_original)):\n",
        "        seq_i = []\n",
        "        for toxic_word in list(set(predicted_toxic_words[i])):\n",
        "            temp = [(m.start(),m.end()) for m in re.finditer(re.escape(toxic_word), lines_original[i])]\n",
        "            for start,end in temp:\n",
        "                seq_i.append(np.arange(start,end))\n",
        "        if(len(seq_i) != 0):\n",
        "            seq_i = set(np.concatenate(seq_i, axis=-1))\n",
        "            seq_i = list((seq_i))\n",
        "            seq_i.sort()\n",
        "        char_positions.append(seq_i)\n",
        "    return char_positions"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbcUGXLZcPPW"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return [1,1,1] if len(predictions)==0 else [0,0,0]\n",
        "    nom = 2*len(set(predictions).intersection(set(gold)))\n",
        "    denom = len(set(predictions))+len(set(gold))\n",
        "    f1 = nom/denom\n",
        "    if len(predictions) == 0:\n",
        "      precision = 0\n",
        "    else:\n",
        "      precision = len(set(predictions).intersection(set(gold)))/len(set(predictions))\n",
        "    recall = len(set(predictions).intersection(set(gold)))/len(set(gold))\n",
        "    return [f1,precision, recall]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f3QHIP9cPPW"
      },
      "source": [
        "class EvaluationMetric(keras.callbacks.Callback):   \n",
        "    \n",
        "    def __init__(self, truncated_trial, trial_original, trial_texts, truncated_trial_masks, lines_original_trial, attention_masks):\n",
        "        super(EvaluationMetric, self).__init__()\n",
        "        self.truncated_trial = truncated_trial\n",
        "        self.trial_original = trial_original\n",
        "        self.trial_texts = trial_texts\n",
        "        self.truncated_trial_masks = truncated_trial_masks\n",
        "        self.lines_original_trial = lines_original_trial\n",
        "        self.attention_masks = attention_masks\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        print(\"\\nTraining...\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print(\"\\nEvaluating...\")\n",
        "        trial_prediction = self.model.predict(self.truncated_trial)\n",
        "        \n",
        "        predicted_labels, predicted_toxic_words = get_predicted_words(trial_prediction, self.trial_texts, self.truncated_trial_masks)\n",
        "        \n",
        "        final = get_char_positions(self.lines_original_trial, predicted_toxic_words)\n",
        "        \n",
        "        sum_f1 = 0\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        for i in range(0,len(final)):\n",
        "            sum_f1 = sum_f1 + f1(final[i], self.trial_original[i])[0]\n",
        "            # print(f1(final[i], self.trial_original[i]))\n",
        "            precision = precision + f1(final[i], self.trial_original[i])[1]\n",
        "            recall = recall + f1(final[i], self.trial_original[i])[2]\n",
        "        \n",
        "        print(\"\\nF1 on val set: \",sum_f1/len(final))\n",
        "        print(\"\\nPrecision on val set: \",precision/len(final))\n",
        "        print(\"\\nRecall on val set: \",recall/len(final))\n",
        "\n",
        "# Comment the evaluation metric while predicting on train set        \n",
        "evaluation_metric = EvaluationMetric(truncated_trial, np.asarray(df_trial[\"spans\"]), trial_texts, truncated_trial_masks, lines_original_trial, attention_masks_trial)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLlitHSncPPW"
      },
      "source": [
        "checkpoint = ModelCheckpoint(filepath='/content/roberta.{epoch:03d}.h5',\n",
        "                                 verbose = 0,\n",
        "                                 save_weights_only=True,\n",
        "                                 epoch=1)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oSz2G3LjJUZ"
      },
      "source": [
        "# Roberta retrain used for visualisation\r\n",
        "history = model.fit(\r\n",
        "    x = truncated_train,\r\n",
        "    y = truncated_train_labels,\r\n",
        "    batch_size=16,\r\n",
        "    shuffle=True,\r\n",
        "    callbacks = [evaluation_metric, checkpoint],\r\n",
        "    epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHs72AI99vbU"
      },
      "source": [
        "<h3>Train Over </h3>\r\n",
        "<p> The next part is for creating results file. Use test file instead of trial file while loading trial data for test set preidictions.</p> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x-wPL_8cPPX"
      },
      "source": [
        "# Test results if you imported test file during initialisation.\n",
        "trial_prediction = model.predict(truncated_trial)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlksm8Q7cPPX"
      },
      "source": [
        "# trial_prediction[3][:20]\n",
        "np.shape(trial_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNECR5RAcPPX"
      },
      "source": [
        "predicted_labels, predicted_toxic_words = get_predicted_words(trial_prediction, trial_texts, truncated_trial_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRyaHVwcPPX"
      },
      "source": [
        "index = 1\n",
        "print(trial_texts[index])\n",
        "print(predicted_labels[index])\n",
        "print(\"Predicted: \",predicted_toxic_words[index])\n",
        "# print(\"True: \",toxic_words[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqizmdNdcPPX"
      },
      "source": [
        "final = get_char_positions(lines_original_trial, predicted_toxic_words)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYCyROtWcPPX"
      },
      "source": [
        "index = 11\n",
        "print(trial_texts[index])\n",
        "print(lines_original_trial[index])\n",
        "# print(predicted_labels[index])\n",
        "print(\"Predicted: \",predicted_toxic_words[index])\n",
        "# print(\"True: \",toxic_words[index])\n",
        "print(\"Predicted: \", final[index])\n",
        "print(\"True: \", df_trial[\"spans\"][index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB_a-9M0cPPY"
      },
      "source": [
        "<h5>Prediction File</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUahyFoscPPY"
      },
      "source": [
        "# make sure that the ids match the ones of the scores\n",
        "predictions = list(final)\n",
        "ids = df_train.index.to_list()\n",
        "\n",
        "# write in a prediction file named \"spans-pred.txt\"\n",
        "with open(\"spans-pred.txt\", \"w\") as out:\n",
        "    for uid, text_scores in zip(ids, predictions):\n",
        "        out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxJUApSOcPPY"
      },
      "source": [
        "! zip -r mpnet_2_high_precision.zip ./spans-pred.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egfiu_J4eSw6"
      },
      "source": [
        "<h4> Analysis </h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF8ygIPlcPPY"
      },
      "source": [
        "sum_f1 = 0\n",
        "precision = 0\n",
        "recall = 0\n",
        "for i in range(0,len(final)):\n",
        "    sum_f1 = sum_f1 + f1(final[i], df_trial[\"spans\"][i])[0]\n",
        "    precision = precision + f1(final[i], df_trial[\"spans\"][i])[1]\n",
        "    recall = recall + f1(final[i], df_trial[\"spans\"][i])[2]\n",
        "\n",
        "print(\"\\nF1 on val set: \",sum_f1/len(final))\n",
        "print(\"\\nPrecision on val set: \",precision/len(final))\n",
        "print(\"\\nRecall on val set: \",recall/len(final))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}